{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"13ZcKsiC6KeSWHHzdBOL1kp1DN9iaxSDQ","timestamp":1609958606842}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6hTB7bGIxcVg"},"source":["# **Aprendizaje supervisado**\n","# SL06. Regresion Logística"]},{"cell_type":"markdown","metadata":{"id":"s3BxVEt5xrq0"},"source":["El algoritmo de **regresión logística** es uno de los más utilizados en aprendizaje automático. Siendo su principal aplicación los problemas de clasificación binaria. Es un algoritmo simple en el que se pueden interpretar fácilmente los resultados obtenidos e identificar por qué se obtiene un resultado u otro. A pesar de su simplicidad funciona muy bien en muchas aplicaciones y se utiliza como referencia de rendimiento.\n","\n","La regresión logística es una técnica de aprendizaje automático que proviene del campo de la estadística. A pesar de su nombre no es un algoritmo para aplicar en problemas de regresión, en los que se busca un valor continuo, sino que es un método para problemas de clasificación, en los que se obtienen un valor binario entre 0 y 1. Por ejemplo, un problema de clasificación es identificar si una operación dada es fraudulenta o no. Asociándole una etiqueta “fraude” a unos registros y “no fraude” a otros. Simplificando mucho es identificar si al realizar una afirmación sobre registro esta es cierta o no.\n","\n","Con la regresión logística se mide la relación entre la variable dependiente, la afirmación que se desea predecir, con una o más variables independientes, el conjunto de características disponibles para el modelo. Para ello utiliza una función logística que determina la probabilidad de la variable dependiente. Como se ha comentado anteriormente, lo que se busca en estos problemas es una clasificación, por lo que la probabilidad se ha de traducir en valores binarios. Para lo que se utiliza un valor umbral. Los valores de probabilidad por encima del valor umbral la afirmación es cierta y por debajo es falsa. Generalmente este valor es 0,5, aunque se puede aumentar o reducir para gestionar el número de falsos positivos o falsos negativos.\n","\n","En términos generales la regresión logísticas usa un **enfoque probabilístico**.\n","\n","$h_{\\theta}(x)$ debería ser $p(y=1/x;\\theta)$.\n","\n","Modelo de regresión logística:\n","\n","$$\n","\\begin{split}\n","h_{\\theta}(x) &= g(\\theta^{T}x)\n","\\\\\n","g(z) &= \\frac{1}{1+e^{-z}}\n","\\\\\n","h_{\\theta}(x) &= \\frac{1}{1+e^{-\\theta^{T}x}}\n","\\end{split}\n","$$\n","\n","![](https://i.ibb.co/NTy2Yh3/reglog.png)\n","\n","$\\theta^{T}x$ debería tener valores **negativos** grandes para instancias negativas y valores **positivos** grandes para instancias positivas.\n","\n","Definir un umbral:\n","\n","* Predecir $y=1$ si $h_{\\theta}(x) \t\\geq0.5$\n","* Predecir $y=0$ si $h_{\\theta}(x) \t< 0.5$\n","\n","![](https://i.ibb.co/NtK5L8R/reglog2.png)"]},{"cell_type":"markdown","metadata":{"id":"Uu7dlyUuyzQY"},"source":["Para entender el funcionamiento de la regresión logística utilizaremos un ejemplo."]},{"cell_type":"markdown","metadata":{"id":"xT_ciuiIM5_v"},"source":["### Cargar el dataset"]},{"cell_type":"code","metadata":{"id":"KiY6bpBRNoGo"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AAK-NTbHN3sn","colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"status":"ok","timestamp":1682522071957,"user_tz":240,"elapsed":262,"user":{"displayName":"Héctor Allende-Cid","userId":"00621000603327740758"}},"outputId":"0d48b415-de0a-4a8d-849c-8ed61ebe3f0f"},"source":["# Data set cargado.\n","\n","iris = load_iris()\n","x = iris.data\n","y = iris.target\n","# Transformelo a data frame para visualizarlo\n","df = pd.DataFrame(x,columns = iris.feature_names)\n","df['species_id'] = y\n","species_map = {0:'setosa',1:'versicolor',2:'virginica'}\n","df['species_name'] = df['species_id'].map(species_map)\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n","0                5.1               3.5                1.4               0.2   \n","1                4.9               3.0                1.4               0.2   \n","2                4.7               3.2                1.3               0.2   \n","3                4.6               3.1                1.5               0.2   \n","4                5.0               3.6                1.4               0.2   \n","\n","   species_id species_name  \n","0           0       setosa  \n","1           0       setosa  \n","2           0       setosa  \n","3           0       setosa  \n","4           0       setosa  "],"text/html":["\n","  <div id=\"df-bf7e1dee-f8fe-4d9b-9aa5-2ae9fcd45f30\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal length (cm)</th>\n","      <th>sepal width (cm)</th>\n","      <th>petal length (cm)</th>\n","      <th>petal width (cm)</th>\n","      <th>species_id</th>\n","      <th>species_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.1</td>\n","      <td>3.5</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>0</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>0</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","      <td>0</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.6</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","      <td>0</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>3.6</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>0</td>\n","      <td>setosa</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf7e1dee-f8fe-4d9b-9aa5-2ae9fcd45f30')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bf7e1dee-f8fe-4d9b-9aa5-2ae9fcd45f30 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bf7e1dee-f8fe-4d9b-9aa5-2ae9fcd45f30');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"Rrwn8qV0Bjel"},"source":["## <font color='green'>Actividad 1</font>\n","\n","Realice un analisis exploratorio con el objetivo de indentificar la factibilidad de construir un clsificador de especies. ¿Qué especie se ve fácilmente separable?"]},{"cell_type":"markdown","metadata":{"id":"ub69qbR5PeLl"},"source":["### Estadistica descriptiva y visualizaciones\n","\n","Hágalo para todas las especies, observe las diferencias.\n","\n","```\n","setosa_mask = df['species_name'] == 'setosa'\n","df[setosa_mask].describe()\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"j5xwgkrX2QHl"},"source":["#Solución\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4eSc6J0-Ps-m"},"source":["### Realice un pair plot\n","\n","Utilice lo visto en cursos anteriores."]},{"cell_type":"markdown","metadata":{"id":"1wpitpRPCGrb"},"source":["<font color='green'>Fin Actividad 1</font>"]},{"cell_type":"markdown","metadata":{"id":"jzht31i0U4E1"},"source":["### Adaptando el data set a un problema binario.\n","\n","Como se vió en las visualizaciones anteriores. Vamos a reconocer setosa de las otras especies. Para esto debemos adaptar nuestra variable de salida.\n","\n","```\n","y1 = np.where(y==2, 1, y) \n","print(y1)\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"l26ZiBhsQvWm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682522403113,"user_tz":240,"elapsed":6,"user":{"displayName":"Héctor Allende-Cid","userId":"00621000603327740758"}},"outputId":"6fb86e96-990f-47f4-b986-f026ee6d1b5b"},"source":["print(y)\n","y1 = np.where(y==2, 1, y) \n","print(y1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2]\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"AmfoMja3P10T"},"source":["### Separar conjunto de entrenamiento y validación\n","\n","Utilice el mismo método que en regresión lineal para realizar split y LogisticRegression para la regresión logistica.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"EI-_n5HAP-HB","colab":{"base_uri":"https://localhost:8080/","height":95},"executionInfo":{"status":"ok","timestamp":1682522406804,"user_tz":240,"elapsed":1026,"user":{"displayName":"Héctor Allende-Cid","userId":"00621000603327740758"}},"outputId":"db89f1c4-2870-4a82-d61b-4ab2312b082e"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression \n","x_train, x_test, y_train, y_test = train_test_split(x, y1, test_size=0.33, random_state=42)\n","log_reg = LogisticRegression()\n","log_reg.fit(x_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression()"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"6243DZ1HVvXJ"},"source":["### Verificación\n","\n","```\n","probabilidades = log_reg.predict_proba(x_test)\n","for i in range(0,len(y_test)):\n","  print (y_test[i],probabilidades[i])\n","\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"jH4eQF1M8Vrx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651781195754,"user_tz":240,"elapsed":297,"user":{"displayName":"Héctor Allende-Cid","userId":"00621000603327740758"}},"outputId":"3232fe60-d3c9-4818-d1d7-91a2dcd11488"},"source":["probabilidades = log_reg.predict_proba(x_test)\n","for i in range(0,10):\n","  print (y_test[i],probabilidades[i])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 [0.00523303 0.99476697]\n","0 [0.9516606 0.0483394]\n","1 [7.76219172e-06 9.99992238e-01]\n","1 [0.00694647 0.99305353]\n","1 [0.00265771 0.99734229]\n","0 [0.95727982 0.04272018]\n","1 [0.0634737 0.9365263]\n","1 [7.72437507e-04 9.99227562e-01]\n","1 [0.00358562 0.99641438]\n","1 [0.02949532 0.97050468]\n"]}]},{"cell_type":"markdown","metadata":{"id":"mg9OY-Q9RglQ"},"source":["## <font color='blue'>**¿Cómo funciona la regresión logística?**</font>\n","\n","Dado un conjunto de entradas X, queremos asignarlas a una de dos categorías posibles (0 o 1). La regresión logística modela la probabilidad de que cada entrada pertenezca a una categoría particular."]},{"cell_type":"markdown","metadata":{"id":"HE5p2nd8S5A6"},"source":["### Contruya una función sigmoidea\n"]},{"cell_type":"code","metadata":{"id":"B4JJRiBnTHRz"},"source":["# Función sigmoidea.\n","\n","def sigmoidea(z):\n","    return 1 / (1 + np.exp(-z))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1niBBU53TL4D"},"source":["### ¿Como medimos que tan bueno está siendo nuestro modelo?\n","### La función de pérdida. Entropía cruzada\n","\n","La entropía de una variable aleatoria $X$, es el nivel de incertidumbre inherente a las posibles variables de resultado.\n","\n","Sea $p(x)$ una distribución de probabilidad de una variable aleatoria $X$. Entonces la **entropía** se define por:\n","<br><br>\n","$$ H(X) =\n","\\begin{cases}\n","- \\int_x p(x) log(p(x)), \\quad \\mbox{para el caso continuo}\n","\\\\\n","- \\sum_x p(x) log(p(x)), \\quad \\mbox{para el caso discreto}\n","\\end{cases}\n","$$\n","<br>\n","![](https://i.ibb.co/BKwcbnV/Log-x.png)\n","\n","Veamos un ejemplo concreto:\n","\n","![Entropia](https://i.ibb.co/qCsnpfC/Ejemplo-Entropia.png) \n","\n","\n","Calcule la entropía para cada uno de los contenedores.\n","\n","1. $ -[\\frac{26}{30}log_2(\\frac{26}{30}) + \\frac{4}{30}log_2(\\frac{4}{30}) ]$ = 0.5665\n","\n","2. =  0.9968\n","\n","3. =  0\n","\n","Nuestra entropía esta midiendo un desorden. Se aplica frecuentemente en termodinámica, mecánica estadística, o teoría de la información (la cantidad de ruido que contiene una señal), etc.\n","\n","## Entropía Cruzada: \n","\n","También se llama __pérdida logarítmica__ o __pérdida logística__. Cada probabilidad de clase predicha se compara con la salida deseada de clase real, 0 o 1, y se calcula una puntuación/pérdida que penaliza la probabilidad en función de qué tan lejos está del valor esperado real. La penalización es de naturaleza logarítmica, lo que genera una puntuación grande para las diferencias grandes cercanas a 1 y una puntuación pequeña para las diferencias pequeñas que tienden a 0.\n","\n","![EntropiaCruzada](https://i.ibb.co/gWHMNgv/Cross-Entropy.png) \n","\n","\n","$$L_{CE} = - \\sum_{i=1}^n t_ilog(p_i)$$, para $n$ clases. \n","\n","Donde $t_i$ es el _truth label_ y $p_i$ la probabilidad obtenida de la función softmax para la $i^{th}$ clase.\n","\n","$$ \\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$$\n","\n","En el caso del ejemplo tenemos:\n","\n","$$L_{CE} = -1log_2(0.936)+0+0+0 = 0.095$$\n","\n","```\n","def loss(h, y):\n","    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"4FkFqMjLXGp1"},"source":["# Función de loss\n","\n","def loss(h, y):\n","    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FJdYTRCRXLOD"},"source":["### El gradiente\n","\n","1. **¿Que es el gradiente?:** El gradiente es un vector que es tangente de una función y apunta en la dirección de mayor aumento de esta función. El gradiente es cero en un máximo o mínimo local porque no hay una única dirección de aumento. En matemáticas, el gradiente se define como una derivada parcial para cada variable de entrada de la función.\n","\n","$$ \\nabla f(x,y) = (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})$$\n","\n","2. **¿Que es el gradiente descendente?:**  Como el gradiente es un vector que apunta al mayor aumento de una función, el gradiente negativo es un vector que apunta al mayor descenso de una función. Por lo tanto, podemos minimizar una función moviéndonos iterativamente **un poco** en la dirección del gradiente negativo. Esa es la lógica del descenso en gradiente.\n","\n","Dado un punto de partida $(X_1^{0}, ...,X_n^{0})$\n","$$ x_1^{(i+1)} = x_1^{(i)} - \\alpha \\; \\frac{\\partial f}{\\partial x_1}(x^{(i)})$$\n","\n","$$ x_n^{(i+1)} = x_n^{(i)} - \\alpha \\; \\frac{\\partial f}{\\partial x_n}(x^{(i)})$$\n","\n","<br>\n","Donde $\\alpha$ corresponde al Learning Rate.\n","\n","\n","![GradientDescent](https://i.ibb.co/6WtB2VK/Gradient-Descent.png) \n","\n","Un parámetro importante en el descenso de gradientes es la tasa de aprendizaje (Learning Rate, $\\alpha$), que determina el tamaño de cada paso. Cuando la tasa de aprendizaje es demasiado grande, el descenso en pendiente puede saltar a través del valle y terminar en el otro lado. Esto conducirá a la divergencia de la función de costos. Por otro lado, cuando la tasa de aprendizaje es demasiado pequeña, el algoritmo tardará mucho en converger. Por lo tanto, se necesita una velocidad de aprendizaje adecuada antes de que comience el descenso de gradiente.\n","\n","\n","![Learningrate](https://i.ibb.co/MMgk5Zj/Learningrate.png)  \n","\n","\n","La normalización juega un papel importante en Gradient Descent. Si las características no están normalizadas, las características a gran escala dominarán la actualización, por lo que el algoritmo generará una ruta de aprendizaje en zigzag. Se necesitan muchos pasos innecesarios y más tiempo para llegar al mínimo. Una vez normalizadas todas las características, la función de coste adquiere una forma más esférica. El algoritmo Gradient Descent va directo al mínimo. Una forma de realizar la normalización es menos la media y dividir por la desviación estándar. También puede aplicar la función Scaler de Scikit-Learn directamente.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XkpBqrgfHQyA"},"source":["## Las ecuaciones:\n","\n","$$h_{\\theta}(x) = \\sigma(\\theta^Tx)$$ \n","Donde $$\\sigma = \\frac{1}{1+ e^{-z}}$$\n","Además sea $$z(\\theta) = \\theta^Tx $$ \n","Se define la función de pérdida (__Loss Function__) de entropía cruzada como:\n","<br><br>\n","$$\n","\\begin{split}\n","L(y,\\hat{y}) &= -[y \\log (\\hat{y}) + (1-y) \\log{(1-\\hat{y})}] \n","\\\\\n","L(y,\\hat{y}) &= -[y \\log (\\sigma(z(\\theta))) + (1-y) \\log{(1-\\sigma(z(\\theta)))}] \n","\\\\\n","\\frac{\\partial L(y,\\hat{y})}{\\partial \\theta_j} &= -\\big[\\frac{\\partial  y\\log (\\sigma(z(\\theta))) }{\\partial \\theta_j} +  \\frac{\\partial (1-y) \\log{(1-\\sigma(z(\\theta)))}}{\\partial \\theta_j}\\big] \n","\\\\\n","\\frac{\\partial L(y,\\hat{y})}{\\partial \\theta_j} &= -\\big[\\frac{y}{\\sigma(z(\\theta))}\\frac{\\partial \\sigma(z(\\theta))}{\\partial \\theta_j} + \\frac{1-y}{1-\\sigma(z(\\theta_j))}\\frac{\\partial \\sigma(1 - z(\\theta))}{\\partial \\theta_j}\\big]\n","\\\\\n","\\frac{\\partial L(y,\\hat{y})}{\\partial \\theta_j} &= \\big[\\frac{1-y}{1-\\sigma(z(\\theta))} -\\frac{y}{\\sigma(z(\\theta))}\\big] \\frac{\\partial \\sigma(z(\\theta))}{\\partial \\theta_j}\n","\\end{split}\n","$$\n","<br><br>\n","Apliquemos la derivada sobre la sigmoidea $\\sigma(z)'= \\sigma(z)(1-\\sigma(z))$\n","<br><br>\n","$$\n","\\begin{split}\n","\\frac{\\partial L(y,\\hat{y})}{\\partial \\theta_j} &= \\big[\\frac{\\sigma(z(\\theta)) - y}{\\sigma(z(\\theta)) [1 - \\sigma(z(\\theta))]} \\big] \\; \\sigma(z(\\theta)) (1-\\sigma(z(\\theta)))x_j\n","\\\\\n","\\frac{\\partial L(y,\\hat{y})}{\\partial \\theta_j} &= [\\sigma(z(\\theta)) - y] x_j\n","\\end{split}\n","$$\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uLAc4scYZBv8"},"source":["\n","\n","\n","```\n","\n","gradiente = np.dot(X.T, (h - y)) / y.shape[0]\n","lr = 0.01\n","theta -= lr * gradient\n","```\n","\n","```\n","def predict_probs(X, theta):\n","    return sigmoid(np.dot(X, theta))\n","```\n","\n","```\n","def add_intercept(X):\n","        intercept = np.ones((X.shape[0], 1))\n","        return np.concatenate((intercept, X), axis=1)\n","```\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bPZCt6SGE2zQ"},"source":["## <font color='green'>Actividad 2</font>\n","\n","Construya la regresión logistica.\n","\n","Defina una funcion fit(X,y):\n","\n","1. Al dataset $X$, se le agrega un intercepto, utilizando la función *add_intercept*.\n","2. Definimos un array theta que contenfa los pesos de la regresión logistica, los puede iniciar en 0 o aleatorios.\n","3. Itere los siguientes pasos una cantidad determinada de veces, por ejemplo 20.000\n","\n","    a.  Haga un forward (multiplique el vector de entrada con theta, utilice la función dot)\n","\n","    b. Aplique la función gradiente.\n","\n","    c. Aplique el learning rate y actualice theta.\n","4. Prediga el conjunto de test con la función predict_probs\n","5. Compare theta y los resultados con la función utilizada en sklearn."]},{"cell_type":"code","metadata":{"id":"RzBKtpqW2Kzf"},"source":["#Solución"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TX2W_R7pShcw"},"source":["<font color='green'>Fin Actividad 2</font>"]}]}